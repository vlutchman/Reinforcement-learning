{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import logger, spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "class CartPoleEnv(gym.Env[np.ndarray, Union[int, np.ndarray]]):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "\n",
    "    ### Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "    ### Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\n",
    "    ### Rewards\n",
    "\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
    "    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "    ### Starting State\n",
    "\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "    ### Episode End\n",
    "\n",
    "    The episode ends if any one of the following occurs:\n",
    "\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "    ### Arguments\n",
    "\n",
    "    ```\n",
    "    gym.make('CartPole-v1')\n",
    "    ```\n",
    "\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "    def __init__(self,nlow = -10.0,nhigh = 10.0,render_mode: Optional[str] = None):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.nhigh = nhigh\n",
    "        self.nlow = nlow\n",
    "        self.action_space = spaces.Box(low=np.array([self.nlow]),high=np.array([self.nhigh]),dtype=np.float64)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        action = np.clip(action,self.action_space.low,self.action_space.high,dtype=np.float32)\n",
    "        force = action.squeeze()\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "            \n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = utils.maybe_parse_reset_bounds(\n",
    "            options, -0.05, 0.05  # default low\n",
    "        )  # default high\n",
    "        self.state = self.np_random.uniform(low=low, high=high, size=(4,))\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gym[classic_control]`\"\n",
    "            )\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode == \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "        \n",
    "    def limits(self):\n",
    "        return self.nlow,self.nhigh\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size, activation = nn.ReLU):\n",
    "        super(ActorCritic,self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size,hidden_size))\n",
    "        layers.append(activation())\n",
    "        layers.append(nn.Linear(hidden_size,hidden_size))\n",
    "        layers.append(activation()) \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        self.mean = nn.Linear(hidden_size,output_size)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(output_size))\n",
    "        self.Value = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out_layers = self.network(x)\n",
    "        mean = self.mean(out_layers)\n",
    "        std = torch.exp(self.log_std)\n",
    "        value = self.Value(out_layers)\n",
    "        return mean,std,value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "timeStep = namedtuple('rollout',('Advantage','state','old_policy','action'))\n",
    "\n",
    "\n",
    "class GradientDataset(Dataset):\n",
    "    def __init__(self, GAE: list,timesteps: list[timeStep]):\n",
    "       self.advantages = GAE\n",
    "       self.timeSteps = timesteps\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.advantages)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        timeStep = self.timeSteps[idx]\n",
    "        return self.advantages[idx], timeStep.old_policy,timeStep.state,timeStep.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinayak Lutchman\\AppData\\Local\\Temp\\ipykernel_17588\\783206616.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Advantage = torch.tensor(reward + gamma*n_value - value,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for 100/400 : 41.0\n",
      "Rewards for 200/400 : 22.0\n",
      "Rewards for 300/400 : 12.0\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data.dataset\n",
    "from collections import namedtuple\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "env = CartPoleEnv(nhigh=10.0,nlow=-10.0,render_mode=\"rgb_array\")\n",
    "\n",
    "batch_size = 100\n",
    "actor = ActorCritic(input_size=env.observation_space.shape[0],hidden_size= env.observation_space.shape[0] +10,output_size=env.action_space.shape[0])\n",
    "num_episodes = 400\n",
    "t = 0\n",
    "t_max = 500\n",
    "low,high = env.limits()\n",
    "state,_ = env.reset()\n",
    "episode = False\n",
    "gamma = 0.99\n",
    "lmb = 0.95\n",
    "LR = 0.001\n",
    "\n",
    "eps = 50\n",
    "\n",
    "trajectory = []\n",
    "\n",
    "optimizer = Adam(actor.parameters(),lr = LR)\n",
    "epsilon = 0.2\n",
    "\n",
    "def GAE_TD(td_errors):\n",
    "  \n",
    "    T = len(td_errors)\n",
    "    advantages = torch.zeros(T)  \n",
    "    gae = 0  \n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        gae = td_errors[t] + gamma * lmb * gae  \n",
    "        advantages[t] = gae  \n",
    "    \n",
    "    return advantages\n",
    "\n",
    "\n",
    "for x in range(num_episodes):\n",
    "    R =0\n",
    "    state,_ = env.reset()\n",
    "    state = torch.tensor(state,dtype=torch.float32)\n",
    "    episode = False\n",
    "    t =0\n",
    "    optimizer.zero_grad()\n",
    "    while not episode and t <t_max:\n",
    "        \n",
    "        mean,std,value = actor(state)\n",
    "        dist = torch.distributions.Normal(mean,std)\n",
    "        action = dist.sample()\n",
    "        action = np.clip(action,a_min = low,a_max=high)\n",
    "        \n",
    "       \n",
    "        \n",
    "        n_state,reward,terminated,truncated,_= env.step(action)\n",
    "        R = R + reward\n",
    "        n_state  = torch.tensor(n_state,dtype=torch.float32)\n",
    "        _,_,n_value = actor(n_state)\n",
    "        \n",
    "        Advantage = torch.tensor(reward + gamma*n_value - value,dtype=torch.float32)\n",
    "        \n",
    "        trajectory.append(timeStep(Advantage=Advantage.detach(),state=state.detach(),old_policy=dist.log_prob(action).detach(),action=action))\n",
    "        \n",
    "        if t%50 == 0 and t!=0:\n",
    "            GAEs = GAE_TD([param.Advantage for param in trajectory])\n",
    "            \n",
    "            dataset = GradientDataset(GAE=GAEs,timesteps=trajectory)\n",
    "            loader = DataLoader(dataset=dataset,batch_size=5,shuffle=False)\n",
    "            for advantages,old_policy,state,action in loader: \n",
    "                             \n",
    "                mean,std,_ = actor(state)\n",
    "                dist = torch.distributions.Normal(mean,std)\n",
    "                new_policy = dist.log_prob(action)\n",
    "                \n",
    "                \n",
    "                ratio = torch.exp(new_policy - old_policy)\n",
    "                \n",
    "                critic_loss = torch.mean(advantages**2)\n",
    "                \n",
    "                clipped = torch.clamp(ratio,min = 1-epsilon,max=1+epsilon)*advantages\n",
    "                \n",
    "                cliploss = torch.mean(torch.min(ratio*advantages,clipped))\n",
    "                \n",
    "                total_loss = cliploss + critic_loss\n",
    "\n",
    "\n",
    "\n",
    "                total_loss.backward()\n",
    "                \n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        state = n_state\n",
    "        \n",
    "     \n",
    "    \n",
    "        \n",
    "        episode = terminated or truncated\n",
    "        t +=1    \n",
    "\n",
    "    if x% 100 == 0 and x != 0:\n",
    "        print(f\"Rewards for {x}/{num_episodes} : {R}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
